[
  {
    "objectID": "notes/example-api.html",
    "href": "notes/example-api.html",
    "title": "OBO API: search and download",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were major breaking changes to the OBO API. The code here is for an example of downloading data from a simple search for one offence with a start and end year.\nSee also the overview of API features and changes.\nThe notes assume some knowledge of R and the Tidyverse. Some of the key packages required are httr, purrr, rvest and xml2.\nThe steps involved are:\n\nbuild a query url\nfetch first page of results\nif more than 1 page of results\n\ncalculate number of queries needed\nbuild the query urls for the extra pages\nfetch the extra pages of results\n\npull trial IDs out of the results\nbuild the per-ID single record URLs\nfetch the single items (the slooooooow bit)\nextract data (xml, text and metadata)\nwrite the xml and text data to individual files\nwrite the metadata table to a csv file\n\nThe XML files downloaded by the code contain the complete (complex) tagging for each trial: offences, verdicts, sentences, names, locations, etc. The TXT files contain the plain text of trials. The metadata is a simplified summary of trial tagging.\nNote: I’m currently thinking about the best methods for running multiple searches when all you want is counts of results and may write another example for this."
  },
  {
    "objectID": "notes/example-api.html#introduction",
    "href": "notes/example-api.html#introduction",
    "title": "OBO API: search and download",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were major breaking changes to the OBO API. The code here is for an example of downloading data from a simple search for one offence with a start and end year.\nSee also the overview of API features and changes.\nThe notes assume some knowledge of R and the Tidyverse. Some of the key packages required are httr, purrr, rvest and xml2.\nThe steps involved are:\n\nbuild a query url\nfetch first page of results\nif more than 1 page of results\n\ncalculate number of queries needed\nbuild the query urls for the extra pages\nfetch the extra pages of results\n\npull trial IDs out of the results\nbuild the per-ID single record URLs\nfetch the single items (the slooooooow bit)\nextract data (xml, text and metadata)\nwrite the xml and text data to individual files\nwrite the metadata table to a csv file\n\nThe XML files downloaded by the code contain the complete (complex) tagging for each trial: offences, verdicts, sentences, names, locations, etc. The TXT files contain the plain text of trials. The metadata is a simplified summary of trial tagging.\nNote: I’m currently thinking about the best methods for running multiple searches when all you want is counts of results and may write another example for this."
  },
  {
    "objectID": "notes/example-api.html#caveats",
    "href": "notes/example-api.html#caveats",
    "title": "OBO API: search and download",
    "section": "Caveats",
    "text": "Caveats\n\ndownloading is not going to be speedy\nFetching trials data is likely to take a while if you have more than a handful of results. It’s advisable to test how many results your search will return on the website before running any of this code. (More generally, the website is the best place to test out and refine queries for the API.)\nThe code below deliberately includes a wait between downloads to be kind to the OBO server. You don’t have to observe this but it will be very much appreciated if you do. (Apart from politeness, abuse of the servers could lead to getting yourself blocked.)\n\n\nhandling problems with queries\nIf you ask for something invalid the API may not always fail in an obvious way (ie, error or 0 results); I recommend always checking the total hits for your query before you run any further code.\nIf you ever see total hits 203163, you have a problem. Basically, this number is the total results in the database; even if it was what you intended (it probably wasn’t), you won’t be able to get all your results because of the search limit (see the next point).\nIf there are 0 results and you expected more, first check your query for typos, then check that the website is up and the same search works there. If either the website or the API server has gone down, you may have no choice but to wait for it to come back (especially if it’s a weekend, sorry). If the problem really persists, especially if there’s no obvious reason for it, send an email.\n\n\nsearch limit\nAs far as I know the API is not rate-limited BUT the search has a hard limit of 10000 results (from=9990). Anything after that causes a server error. If your search has more than 10000 results you’ll have to split it up into several smaller searches (eg by date) and combine again afterwards."
  },
  {
    "objectID": "notes/example-api.html#load-packages-and-functions",
    "href": "notes/example-api.html#load-packages-and-functions",
    "title": "OBO API: search and download",
    "section": "Load packages and functions",
    "text": "Load packages and functions\n\nlibrary(here)\n\nhere() starts at /Users/vanity/r_projects/my_websites/obo\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(glue)\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(xml2)\nlibrary(rvest)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()         masks stats::filter()\n✖ purrr::flatten()        masks jsonlite::flatten()\n✖ readr::guess_encoding() masks rvest::guess_encoding()\n✖ dplyr::lag()            masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## functions\n\n# fetch json using jsonlite::fromJSON(), but slow it down with purrr::slowly() to be kind to the OBO server\nslow_json &lt;- \n  slowly(fromJSON, rate = rate_delay(4))\n\n\n## write xml files in a named folder (using the offence search term), one trial per file\n## NB: the function requires a folder named outputs/ in the project root   \n## but it will create subfolders if they don't already exist\n## it will be used inside purr::map(). the approach is adapted from \n## https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/\n\noutput_obo_xml &lt;- function(data, names) {\n    \n  # using here() from the [here](https://here.r-lib.org/) package for file referencing\n  \n    folder_path &lt;- here::here(\"outputs/xml\") \n    if (!dir.exists(folder_path)) {dir.create(folder_path)}\n    \n    folder_path_query &lt;- paste(folder_path, query_offence, sep = \"/\")\n    if (!dir.exists(folder_path_query)) {dir.create(folder_path_query)}\n    \n    write_xml(data, paste0(folder_path_query, \"/\", names, \".xml\"), options=c(\"format\"))\n}\n\n# ditto, but for plain text files\noutput_obo_txt &lt;- function(data, names) {\n     \n    folder_path &lt;- here::here(\"outputs/txt\") \n    if (!dir.exists(folder_path)) {dir.create(folder_path)}\n    \n    folder_path_query &lt;- paste(folder_path, query_offence, sep = \"/\")\n    if (!dir.exists(folder_path_query)) {dir.create(folder_path_query)}\n    \n    write_file(data, paste0(folder_path_query, \"/\", names, \".txt\") )\n}\n\n\n## turn \"metadata\" into a table\n# this is the summary info that appears at the top of each trial on the website\n# easier to work with than the xml  \n# BUT limited usefulness for complex trials (multiple defts/offences/verdicts/sentences).\n\nobo_metadata_table &lt;- function(data){\n  data |&gt;\n  html_table() |&gt;\n  # pivot complains it's a list if you don't do this\n  as.data.frame() |&gt; \n  pivot_wider(names_from = X1, values_from = X2) |&gt;\n  clean_names(\"snake\") |&gt;\n  select(-navigation)\n}"
  },
  {
    "objectID": "notes/example-api.html#set-up-the-query",
    "href": "notes/example-api.html#set-up-the-query",
    "title": "OBO API: search and download",
    "section": "Set up the query",
    "text": "Set up the query\nThis example uses housebreaking between 1700 and 1704 (14 trials).\nSee the API documentation for the basics. But that is very basic; the aim of this example is to help fill in some of the gaps.\nAs a general rule, it helps to know that API queries mirror website searches.\nThe website search URL for the example (first ten results):\nhttps://www.oldbaileyonline.org/search/crime?offence=housebreaking&year_gte=1700&year_lte=1704#results\nThe matching API query URL:\nhttps://www.dhi.ac.uk/api/data/oldbailey_record?offence=housebreaking&year_gte=1700&year_lte=1704\n\n# query_offence will be reused for the name of the folder to save files\nquery_offence &lt;- \"housebreaking\"\nquery_start_year &lt;- \"1700\"\nquery_end_year &lt;- \"1704\"\n\n# parse the search endpoint URL into its components\nquery_url &lt;-\n  parse_url(\"https://www.dhi.ac.uk/api/data/oldbailey_record\")\n\n# add the query parameters\nquery_url$query &lt;- \n  list(offence=query_offence,\n       year_gte=query_start_year,\n       year_lte=query_end_year \n       )\n\n# turn it back into a URL incorporating the parameters\nsearch_url &lt;- build_url(query_url)\n\n\n# endpoint url for single records (this doesn't need parsing)\nsingle_url &lt;- \"https://www.dhi.ac.uk/api/data/oldbailey_record_single\"\n\n\n# run the search to get first page of results\nsearch_json &lt;-\n  fromJSON(search_url)\n\n\n# how many hits?\nsearch_hits &lt;-\n  search_json$hits$total\n\n# how many pages is that?\nsearch_pages &lt;-\n  ceiling(search_hits/10)\n\nIt’s a good idea to check that the number of results looks like what you expect before you start processing anything\n\nsearch_hits\n\n[1] 14"
  },
  {
    "objectID": "notes/example-api.html#download-trials-and-save-results",
    "href": "notes/example-api.html#download-trials-and-save-results",
    "title": "OBO API: search and download",
    "section": "Download trials and save results",
    "text": "Download trials and save results\nThis builds in a check to stop the process if there are either 0 results or more than 10,000 results (see note above on this API limit). As already noted, if your search has more than 10,000 results you’ll have to split it up into smaller slices.\n\n# if 0 or &gt;10000 results stop right now\n\nif (search_pages==0 | search_pages&gt;1000) {\n   \n  \"check number of results!\"\n  \n# if 1-10000 results carry on...\n  \n} else {\n  \n  \n# extract IDs from the first page\n  \nsearch_ids_page1 &lt;-\nsearch_json$hits$hits$`_source` |&gt;\n      select(idkey, title, text)\n\n\n  # if only 1 page of results, just get IDs for those and make single record API URLs\n\n  if(search_pages ==1) {\n    \n  search_ids &lt;-\n    search_ids_page1 |&gt;\n    mutate(url = glue(\"{single_url}?idkey={idkey}\"))\n  \n  # if &gt;1 page of results, fetch the extra pages, pull out IDs, make URLs and combine with the first lot\n  \n  } else {\n    \n  search_pages_from &lt;-\n  # expand to sequence from 2:search_pages (don't need 1 as you already have the first page)\n  seq(2, search_pages) |&gt;\n  # put each page on a row in a df \n  enframe(name=NULL, value = \"pagenum\") |&gt;\n  # calculate from= value\n  mutate(from = (pagenum-1)*10) |&gt;\n  # append from= to the search_url \n  mutate(url = glue(\"{search_url}&from={from}\"))\n\n\n  # run the new query. this could take a little while if there are a lot of pages.\n  search_json_pages &lt;-\n   map(search_pages_from$url, slow_json)\n\n\n  # extract the IDs from the json, which is more deeply nested than the first. \n  # this code could probably be improved because my grasp of this stuff is a bit ropey\n  # I make extensive use of the purrr lessons at https://jennybc.github.io/purrr-tutorial/index.html\n  search_ids_pages &lt;-\n  map(search_json_pages, `[`, \"hits\" ) |&gt;\n  map(\"hits\") |&gt; # or could use flatten()\n  # bind_rows flattens into a single df with hits as a list-column. i don't know why, it just does.\n  bind_rows() |&gt;\n  unnest(hits) |&gt;\n  unnest(`_source`) |&gt;\n  select(idkey, title, text)\n\n  # bind first page to extras\n  search_ids &lt;-\n  bind_rows(\n    search_ids_page1,\n    search_ids_pages\n  ) |&gt;\n  mutate(url = glue(\"{single_url}?idkey={idkey}\"))\n\n  }\n  # end of nested if/else\n  \n\n# fetch the trials data. **this bit will take a while**\nfetch_ids &lt;-\n  map(search_ids$url, slow_json)\n\n\n# extract the good stuff\nresults_ids &lt;-\nmap(fetch_ids, `[`, \"hits\" ) |&gt;\n  map(\"hits\") |&gt;\n  bind_rows() |&gt;\n  unnest(hits) |&gt;\n  unnest(`_source`) |&gt;\n  select(idkey, metadata, title, xml, text)\n\n\n## save the data to files so you don't have to run queries again\n## need to parse the metadata/xml fields as xml/html (xml2::read_xml / rvest::read_html)\n\n\n# list of ids to use as filenames\n# (I prefer to redo this using the downloaded data rather than reusing earlier list of IDs)\nresults_ids_names &lt;-\n  results_ids$idkey\n\n# parse the xml\nresults_xml &lt;-\n  map(results_ids$xml, read_xml)\n\n# select the plain text\nresults_txt &lt;-\n  results_ids$text\n\n\n## write files using the output functions above\n# note use of purrr::pmap() https://purrr.tidyverse.org/reference/pmap.html\n# invisible() stops printing out of console messages which can get a bit much after the n-hundredth time\n\n# why is this not working?? check function agt original i suppose\n\n# write xml\ninvisible(\n  list(\n    data=results_xml,\n    names=results_ids_names\n  ) |&gt;\n  pmap(output_obo_xml)\n )\n\n# write plain text\ninvisible(\n  list(\n    data=results_txt,\n    names=results_ids_names\n  ) |&gt;\n  pmap(output_obo_txt)\n )\n\n\n# save the \"metadata\" for each trial in a CSV file. \n\nresults_metadata &lt;-\nmap(results_ids$metadata, read_html) |&gt; \n  map(obo_metadata_table) |&gt;\n  bind_rows()\n\nwrite_csv(results_metadata, paste0(here::here(\"outputs\"), \"/\", query_offence, \".csv\"), na=\"\")\n\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Old Bailey Online for Data Analysis",
    "section": "",
    "text": "This website is a resource for users of the Old Bailey Online who are interested in Digital Humanities, Data Science and computational approaches to the OBO’s complex data.\nI plan to write guides to using the OBO API, as well as useful tools and methods for processing data obtained via the API."
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Old Bailey Online for Data Analysis",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "notes/intro-api.html",
    "href": "notes/intro-api.html",
    "title": "The new OBO API: an overview",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were major breaking changes to the OBO API. The available documentation for the new API is sparse, and these notes are intended to help users find their way around.\nI cover two of the available API endpoints:\n\nhttps://www.dhi.ac.uk/api/data/oldbailey_record - for searching trials and other sections in the Old Bailey Proceedings\nhttps://www.dhi.ac.uk/api/data/oldbailey_record_single - for full data about individual trials and other sections in the Proceedings\n\nThere are two other endpoints, one for Ordinary’s Accounts and one for the Associated Records database. I haven’t yet looked at these in any detail and may add further notes about them later, but would expect them to be similar.\nSee also this worked example to search and download data from the API using R."
  },
  {
    "objectID": "notes/intro-api.html#introduction",
    "href": "notes/intro-api.html#introduction",
    "title": "The new OBO API: an overview",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were major breaking changes to the OBO API. The available documentation for the new API is sparse, and these notes are intended to help users find their way around.\nI cover two of the available API endpoints:\n\nhttps://www.dhi.ac.uk/api/data/oldbailey_record - for searching trials and other sections in the Old Bailey Proceedings\nhttps://www.dhi.ac.uk/api/data/oldbailey_record_single - for full data about individual trials and other sections in the Proceedings\n\nThere are two other endpoints, one for Ordinary’s Accounts and one for the Associated Records database. I haven’t yet looked at these in any detail and may add further notes about them later, but would expect them to be similar.\nSee also this worked example to search and download data from the API using R."
  },
  {
    "objectID": "notes/intro-api.html#notes-on-search-parameters",
    "href": "notes/intro-api.html#notes-on-search-parameters",
    "title": "The new OBO API: an overview",
    "section": "Notes on search parameters",
    "text": "Notes on search parameters\nThe official documentation only mentions keyword searching with the text parameter. These notes are intended to fill in some of the gaps for more structured searches of offences, etc.\nThe crucial thing to know is that API query URLs mirror search results URLs on the website.\nSo, for example, the URL for the first page of results for a site search for the offence “killing”:\n\nhttps://www.oldbaileyonline.org/search/crime?offence=kill\n\nThe corresponding api query url:\n\nhttps://www.dhi.ac.uk/api/data/oldbailey_record?offence=kill\n\n(Note: the /crime/ part of the website URL is not needed.)\nSo generally you should be able to test out queries on the website first to work out what parameters you need for API queries, and I’m not going to list a lot of examples here.\n(There could potentially be specific omissions in the API that I’m not aware of, so I recommend initially testing out results for whatever you want to do.)\n\npagination\nThe example URL above will only get the first 10 results, but most searches will return more than that. The parameter for this is from.\nTo get the second page of results: from=10\nSo, for the example above: https://www.dhi.ac.uk/api/data/oldbailey_record?offence=kill&from=10\n(Note that numbering starts from 0, not 1.)\n\n\ndates and date ranges\nA range specifying month and year (March 1739-August 1755): month_gte=3&month_lte=8&year_gte=1739&year_lte=1755\nFor the year range only: year_gte=1739&year_lte=1755\nA single year: year_gte=1739&year_lte=1739\n\n\noffences\nThe top level category breaking peace (including all subcategories): offence=breakingPeace\nFor wounding, subcategory of breakingPeace: offence=wounding\nFor subcategories you don’t need to specify the parent category except for “other” subcategories\n\noffence=breakingPeaceOther / offence=deceptionNoDetail\n\n“NoDetail” is a new subcategory which essentially means the same thing as “Other” but reflects some inconsistencies in the XML where the offence was tagged without any subcategory at all (but should properly have been Other). In the old API I think there was no way to pull these out separately.\n\n\nverdicts and sentences\nA verdict of guilty: verdict=guilty\nA sentence of death: punishment=death\n\n\npleas (new)\nPleaded guilty: plea=guilty\nI think all the old “pleaded” verdicts are still available under verdict and on a superficial inspection seem to get much the same results. Eg plea=guilty and verdict=pleadedGuilty seem to get the same results. However, this new option is the result of a fairly recent project and there may well be details I don’t know about in the background information on the website.\n\n\nitem types\nTo restrict to trial texts only (ie excluding advertisements, supplementary, etc): div_type=trialAccount\nThis probably won’t matter when doing offence/verdict/sentence queries but will be more relevant for text queries.\n\n\ndefendants and victims\nEg for gender:\n\ndefendant_gender=female\nvictim_gender=male"
  },
  {
    "objectID": "notes/intro-api.html#the-json",
    "href": "notes/intro-api.html#the-json",
    "title": "The new OBO API: an overview",
    "section": "The JSON",
    "text": "The JSON\nThere is a lot of… stuff… in the JSON returned by the API, much of it not useful for data analysis. This focuses on some bits you might care about.\n\nsearch results\nBrowse a sample of the search results JSON:\n\njsonedit(sample_search_json)\n\n\n\n\n\n(search for housebreaking between 1700 and 1704)\nThe number of results (hits &gt; total)\n\nsample_search_json$hits$total\n\n[1] 14\n\n\nTrial IDs (hits &gt; hits &gt; _source &gt; idkey) - needed to get full data of individual trials via the single record endpoint.\n\nsample_search_json$hits$hits$`_source`$idkey\n\n [1] \"t17000115-17\" \"t17000115-21\" \"t17000828-4\"  \"t17000828-13\" \"t17000828-14\"\n [6] \"t17000828-28\" \"t17000828-29\" \"t17021014-5\"  \"t17021014-9\"  \"t17021014-19\"\n\n\nThe search returns very limited information about each trial (or other item); it does include a text snippet and the page title for each item, which could be useful for some purposes.\n\nsample_search_json$hits$hits$`_source`$text[[1]]\n\n[1] \"Anne Buckler and Elizabeth Jefferys , both of the Parish of St. Andrews Holborne , the first was Indicted as Principal, and the other as Accessary before the Fact committed, for breaking the House of Thomas Beckenson , on the 9th of December last, with an Intention to steal his Goods . The Evidence not being sufficient to Convict the Principal they were both acquitted .\"\n\n\n\nsample_search_json$hits$hits$`_source`$title[[1]]\n\n[1] \"Anne Buckler. Elizabeth Jefferys. Theft; housebreaking. 15th January 1700.\"\n\n\n\n\nsingle records\nThe single record endpoint is needed to get the full data about items.\nIt includes three versions of the trial/item text; two are most likely to be of interest for data analysis.\n\nxml - the full tagged XML (hits &gt; hits &gt; _source &gt; xml)\ntext - a plain text version (hits &gt; hits &gt; _source &gt; text)\n\nplus\n\nmetadata - this contains the same information as the simplified summary table at the top of each trial page, which might be sufficient for some purposes (hits &gt; hits &gt; _source &gt; metadata)\n\nBrowse a sample trial JSON:\n\njsonedit(sample_trial_json)\n\n\n\n\n\n\n## sample_trial_json$hits$hits$`_source`$xml"
  },
  {
    "objectID": "notes/intro-api.html#for-users-of-the-old-obapi",
    "href": "notes/intro-api.html#for-users-of-the-old-obapi",
    "title": "The new OBO API: an overview",
    "section": "For users of the old OBAPI",
    "text": "For users of the old OBAPI\nA few notes about the major differences between new and old API.\n\nchanges to query URLs\nPreviously for offences/verdicts/sentences in the URL it was always necessary to explicitly spell out category_subcategory. That’s no longer necessary except in a few specific contexts (and it’s done slightly differently).\nAn example: offence “fraud”, subcategory of deception. The relevant bit of a search URL previously looked like this:\n_offences_offenceCategory_offenceSubcategory=deception_fraud\nThat’s been replaced by the much shorter and simpler\noffence=fraud\nNow the top level category is only needed if\n\nsearching for the whole category, eg: offence=deception\nsearching for subcategories Other or NoDetail, which need to be like this\n\noffence=deceptionOther (previously deception_other)\noffence=deceptionNoDetail (not in the previous version)\n\n\n\n\nwhat’s gone away\nThe new API is in many ways more powerful than the old and with fewer limits, but it’s also more generic and some functionality is no longer available.\n\nthe terms endpoint which gave a list of all the fields and values available in the API\nthe option (breakdown=[field]) for a breakdown of subcategories and so on for a search\nthe option (return=zip) to download zip archives of multiple trials XML\n\nThe functionality can be reproduced but usually with some loss of convenience; if you have scripts using them you’ll probably find that their replacements need to be longer and more complicated.\nThe OBAPI demonstrator has also been withdrawn; this has no replacement."
  }
]