[
  {
    "objectID": "notes/example-api.html",
    "href": "notes/example-api.html",
    "title": "OBO API: search and download",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were substantial breaking changes to the OBO API. The code here is an example of downloading data from a simple search for one offence with a start and end year.\nSee also the overview of API features and changes.\nThe notes assume some knowledge of R and the Tidyverse. Some of the key packages required are httr, purrr, rvest and xml2.\nThe steps involved are:\n\nbuild a query url\nfetch first page of results\nif more than 1 page of results\n\ncalculate number of queries needed\nbuild the query urls for the extra pages\nfetch the extra pages of results\n\npull trial IDs out of the results\nbuild the per-ID single record URLs\nfetch the single items (the slooooooow bit)\nextract data (xml, text and metadata)\nwrite the xml and text data to individual files\nwrite the metadata table to a csv file\n\nThe XML files downloaded by the code contain the complete (complex) tagging for each trial: offences, verdicts, sentences, names, locations, etc. The TXT files contain the plain text of trials. The metadata is a simplified summary of trial tagging."
  },
  {
    "objectID": "notes/example-api.html#introduction",
    "href": "notes/example-api.html#introduction",
    "title": "OBO API: search and download",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were substantial breaking changes to the OBO API. The code here is an example of downloading data from a simple search for one offence with a start and end year.\nSee also the overview of API features and changes.\nThe notes assume some knowledge of R and the Tidyverse. Some of the key packages required are httr, purrr, rvest and xml2.\nThe steps involved are:\n\nbuild a query url\nfetch first page of results\nif more than 1 page of results\n\ncalculate number of queries needed\nbuild the query urls for the extra pages\nfetch the extra pages of results\n\npull trial IDs out of the results\nbuild the per-ID single record URLs\nfetch the single items (the slooooooow bit)\nextract data (xml, text and metadata)\nwrite the xml and text data to individual files\nwrite the metadata table to a csv file\n\nThe XML files downloaded by the code contain the complete (complex) tagging for each trial: offences, verdicts, sentences, names, locations, etc. The TXT files contain the plain text of trials. The metadata is a simplified summary of trial tagging."
  },
  {
    "objectID": "notes/example-api.html#caveats",
    "href": "notes/example-api.html#caveats",
    "title": "OBO API: search and download",
    "section": "Caveats",
    "text": "Caveats\n\ndownloading trials is not going to be speedy\nFetching trials data is likely to take a while if you have more than a handful of results. It’s advisable to test how many results your search will return on the website before running any of this code. (More generally, the website is the best place to test out and refine queries for the API.)\nThe code below deliberately includes a wait between downloads to be kind to the OBO server. You don’t have to observe this but it will be very much appreciated if you do. (Apart from politeness, abuse of the servers could lead to getting yourself blocked.)\n\n\nhandling problems with queries\nIf you ask for something invalid the API may not always fail in an obvious way (ie, error or 0 results); I recommend always checking the total hits for your query before you run any further code.\nIf you ever see total hits 203163, you have a problem. Basically, this number is the total results in the database; even if it was what you intended (it probably wasn’t), you won’t be able to get all your results because of the search limit (see the next point).\nIf there are 0 results and you expected more, first check your query for typos, then check that the website is up and the same search works there. If either the website or the API server has gone down, you may have no choice but to wait for it to come back (especially if it’s a weekend, sorry). If the problem really persists, especially if there’s no obvious reason for it, send an email.\n\n\nsearch limit\nAs far as I know the API is not rate-limited BUT the search has a hard limit of 10000 results (from=9990). Anything after that causes a server error. If your search has more than 10000 results you’ll have to split it up into several smaller searches (eg by date) and combine again afterwards."
  },
  {
    "objectID": "notes/example-api.html#load-packages-and-functions",
    "href": "notes/example-api.html#load-packages-and-functions",
    "title": "OBO API: search and download",
    "section": "Load packages and functions",
    "text": "Load packages and functions\n\nlibrary(here)\n\nhere() starts at /Users/vanity/r_projects/my_websites/obo\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(glue)\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(xml2)\nlibrary(rvest)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()         masks stats::filter()\n✖ purrr::flatten()        masks jsonlite::flatten()\n✖ readr::guess_encoding() masks rvest::guess_encoding()\n✖ dplyr::lag()            masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## functions\n\n# fetch json using jsonlite::fromJSON(), but slow it down with purrr::slowly() to be kind to the OBO server\nslow_json &lt;- \n  slowly(fromJSON, rate = rate_delay(4))\n\n\n## write xml files in a named folder (using the offence search term), one trial per file\n## NB: the function requires a folder named outputs/ in the project root   \n## but it will create subfolders if they don't already exist\n## it will be used inside purr::map(). the approach is adapted from \n## https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/\n\noutput_obo_xml &lt;- function(data, names) {\n    \n  # using here() from the [here](https://here.r-lib.org/) package for file referencing\n  \n    folder_path &lt;- here::here(\"outputs/xml\") \n    if (!dir.exists(folder_path)) {dir.create(folder_path)}\n    \n    folder_path_query &lt;- paste(folder_path, query_offence, sep = \"/\")\n    if (!dir.exists(folder_path_query)) {dir.create(folder_path_query)}\n    \n    write_xml(data, paste0(folder_path_query, \"/\", names, \".xml\"), options=c(\"format\"))\n}\n\n# ditto, but for plain text files\noutput_obo_txt &lt;- function(data, names) {\n     \n    folder_path &lt;- here::here(\"outputs/txt\") \n    if (!dir.exists(folder_path)) {dir.create(folder_path)}\n    \n    folder_path_query &lt;- paste(folder_path, query_offence, sep = \"/\")\n    if (!dir.exists(folder_path_query)) {dir.create(folder_path_query)}\n    \n    write_file(data, paste0(folder_path_query, \"/\", names, \".txt\") )\n}\n\n\n## turn \"metadata\" into a table\n# this is the summary info that appears at the top of each trial on the website\n# easier to work with than the xml  \n# BUT limited usefulness for complex trials (multiple defts/offences/verdicts/sentences).\n\nobo_metadata_table &lt;- function(data){\n  data |&gt;\n  html_table() |&gt;\n  # pivot complains it's a list if you don't do this\n  as.data.frame() |&gt; \n  pivot_wider(names_from = X1, values_from = X2) |&gt;\n  clean_names(\"snake\") |&gt;\n  select(-navigation)\n}"
  },
  {
    "objectID": "notes/example-api.html#set-up-the-query",
    "href": "notes/example-api.html#set-up-the-query",
    "title": "OBO API: search and download",
    "section": "Set up the query",
    "text": "Set up the query\nThis example uses housebreaking between 1700 and 1704 (14 trials).\nSee the API documentation for the basics. But that is very basic; the aim of this example is to help fill in some of the gaps.\nAs a general rule, it helps to know that API queries should mirror website searches.\nThe website search URL for the example (first ten results):\nhttps://www.oldbaileyonline.org/search/crime?offence=housebreaking&year_gte=1700&year_lte=1704#results\nThe matching API query URL:\nhttps://www.dhi.ac.uk/api/data/oldbailey_record?offence=housebreaking&year_gte=1700&year_lte=1704\n\n# query_offence will be reused for the name of the folder to save files\nquery_offence &lt;- \"housebreaking\"\nquery_start_year &lt;- \"1700\"\nquery_end_year &lt;- \"1704\"\n\n# parse the search endpoint URL into its components\nquery_url &lt;-\n  parse_url(\"https://www.dhi.ac.uk/api/data/oldbailey_record\")\n\n# add the query parameters\nquery_url$query &lt;- \n  list(offence=query_offence,\n       year_gte=query_start_year,\n       year_lte=query_end_year \n       )\n\n# turn it back into a URL incorporating the parameters\nsearch_url &lt;- build_url(query_url)\n\n\n# endpoint url for single records (this doesn't need parsing)\nsingle_url &lt;- \"https://www.dhi.ac.uk/api/data/oldbailey_record_single\"\n\n\n# run the search to get first page of results\nsearch_json &lt;-\n  fromJSON(search_url)\n\n\n# how many hits?\nsearch_hits &lt;-\n  search_json$hits$total\n\n# how many pages is that?\nsearch_pages &lt;-\n  ceiling(search_hits/10)\n\nIt’s a good idea to check that the number of results looks like what you expect before you start processing anything\n\nsearch_hits\n\n[1] 14"
  },
  {
    "objectID": "notes/example-api.html#download-trials-and-save-results",
    "href": "notes/example-api.html#download-trials-and-save-results",
    "title": "OBO API: search and download",
    "section": "Download trials and save results",
    "text": "Download trials and save results\nThis builds in a check to stop the process if there are either 0 results or more than 10,000 results (see note above on this API limit). As already noted, if your search has more than 10,000 results you’ll have to split it up into smaller slices.\n\n# if 0 or &gt;10000 results stop right now\n\nif (search_pages==0 | search_pages&gt;1000) {\n   \n  \"check number of results!\"\n  \n# if 1-10000 results carry on...\n  \n} else {\n  \n  \n# extract IDs from the first page\n  \nsearch_ids_page1 &lt;-\nsearch_json$hits$hits$`_source` |&gt;\n      select(idkey, title, text)\n\n\n  # if only 1 page of results, just get IDs for those and make single record API URLs\n\n  if(search_pages ==1) {\n    \n  search_ids &lt;-\n    search_ids_page1 |&gt;\n    mutate(url = glue(\"{single_url}?idkey={idkey}\"))\n  \n  # if &gt;1 page of results, fetch the extra pages, pull out IDs, make URLs and combine with the first lot\n  \n  } else {\n    \n  search_pages_from &lt;-\n  # expand to sequence from 2:search_pages (don't need 1 as you already have the first page)\n  seq(2, search_pages) |&gt;\n  # put each page on a row in a df \n  enframe(name=NULL, value = \"pagenum\") |&gt;\n  # calculate from= value\n  mutate(from = (pagenum-1)*10) |&gt;\n  # append from= to the search_url \n  mutate(url = glue(\"{search_url}&from={from}\"))\n\n\n  # run the new query. this could take a little while if there are a lot of pages.\n  search_json_pages &lt;-\n   map(search_pages_from$url, slow_json)\n\n\n  # extract the IDs from the json, which is more deeply nested than the first. \n  # this code could probably be improved because my grasp of this stuff is a bit ropey\n  # I make extensive use of the purrr lessons at https://jennybc.github.io/purrr-tutorial/index.html\n  search_ids_pages &lt;-\n  map(search_json_pages, `[`, \"hits\" ) |&gt;\n  map(\"hits\") |&gt; # or could use flatten()\n  # bind_rows flattens into a single df with hits as a list-column. i don't know why, it just does.\n  bind_rows() |&gt;\n  unnest(hits) |&gt;\n  unnest(`_source`) |&gt;\n  select(idkey, title, text)\n\n  # bind first page to extras\n  search_ids &lt;-\n  bind_rows(\n    search_ids_page1,\n    search_ids_pages\n  ) |&gt;\n  mutate(url = glue(\"{single_url}?idkey={idkey}\"))\n\n  }\n  # end of nested if/else\n  \n\n# fetch the trials data. **this bit will take a while**\nfetch_ids &lt;-\n  map(search_ids$url, slow_json)\n\n\n# extract the good stuff\nresults_ids &lt;-\nmap(fetch_ids, `[`, \"hits\" ) |&gt;\n  map(\"hits\") |&gt;\n  bind_rows() |&gt;\n  unnest(hits) |&gt;\n  unnest(`_source`) |&gt;\n  select(idkey, metadata, title, xml, text)\n\n\n## save the data to files so you don't have to run queries again\n## need to parse the metadata/xml fields as xml/html (xml2::read_xml / rvest::read_html)\n\n\n# list of ids to use as filenames\n# (I prefer to redo this using the downloaded data rather than reusing earlier list of IDs)\nresults_ids_names &lt;-\n  results_ids$idkey\n\n# parse the xml\nresults_xml &lt;-\n  map(results_ids$xml, read_xml)\n\n# select the plain text\nresults_txt &lt;-\n  results_ids$text\n\n\n## write files using the output functions above\n# note use of purrr::pmap() https://purrr.tidyverse.org/reference/pmap.html\n# invisible() stops printing out of console messages which can get a bit much after the n-hundredth time\n\n# why is this not working?? check function agt original i suppose\n\n# write xml\ninvisible(\n  list(\n    data=results_xml,\n    names=results_ids_names\n  ) |&gt;\n  pmap(output_obo_xml)\n )\n\n# write plain text\ninvisible(\n  list(\n    data=results_txt,\n    names=results_ids_names\n  ) |&gt;\n  pmap(output_obo_txt)\n )\n\n\n# save the \"metadata\" for each trial in a CSV file. \n\nresults_metadata &lt;-\nmap(results_ids$metadata, read_html) |&gt; \n  map(obo_metadata_table) |&gt;\n  bind_rows()\n\nwrite_csv(results_metadata, paste0(here::here(\"outputs\"), \"/\", query_offence, \".csv\"), na=\"\")\n\n}"
  },
  {
    "objectID": "notes/statistics-terms.html",
    "href": "notes/statistics-terms.html",
    "title": "API terms and statistics",
    "section": "",
    "text": "There were a couple of convenient features in the old OBAPI whose absence I’ve been asked about a number of times since the API changed. The first was the terms endpoint, which returned a JSON list of all the fields that could be queried, their types and possible values. The second was the breakdown parameter which could be added to API queries to include a frequency table of terms in the results, and was very handy for statistical analysis.\nUntil very recently I thought there was no way to reproduce either functionality in the new API without constructing lists manually and/or running a lot of queries using the main search endpoint. And, as far as I know, there aren’t any exact equivalents. But it turns out you can get (most of) the same information, plus more that wasn’t queryable before.\nHowever, it’s undocumented and not easy to use without some background knowledge about the underlying data and how search functions work. So I’m writing this to fill in the gaps (and provide a resource that I can point enquirers to in the future).\nNote that I’m still learning about the functions and this post may be subject to minor revisions."
  },
  {
    "objectID": "notes/statistics-terms.html#introduction",
    "href": "notes/statistics-terms.html#introduction",
    "title": "API terms and statistics",
    "section": "",
    "text": "There were a couple of convenient features in the old OBAPI whose absence I’ve been asked about a number of times since the API changed. The first was the terms endpoint, which returned a JSON list of all the fields that could be queried, their types and possible values. The second was the breakdown parameter which could be added to API queries to include a frequency table of terms in the results, and was very handy for statistical analysis.\nUntil very recently I thought there was no way to reproduce either functionality in the new API without constructing lists manually and/or running a lot of queries using the main search endpoint. And, as far as I know, there aren’t any exact equivalents. But it turns out you can get (most of) the same information, plus more that wasn’t queryable before.\nHowever, it’s undocumented and not easy to use without some background knowledge about the underlying data and how search functions work. So I’m writing this to fill in the gaps (and provide a resource that I can point enquirers to in the future).\nNote that I’m still learning about the functions and this post may be subject to minor revisions."
  },
  {
    "objectID": "notes/statistics-terms.html#discovery",
    "href": "notes/statistics-terms.html#discovery",
    "title": "API terms and statistics",
    "section": "Discovery",
    "text": "Discovery\nThe solution to both problems lies in the statistics search. When you run a search there, you can see a “Download this data” link at the bottom of the table or visualisation it displays; if you click on the link it’ll download a CSV file. That’s pretty useful.\nBut it gets better when you inspect the URL in the download link. (If you’re not familiar with doing that, the Programming Historian has a quick primer.)\nThis is the download URL for a default offence categories search:\n\nhttps://www.dhi.ac.uk/api/data/csv/agg/oldbailey_offence?series=offence_category\n\nIt looks very much like an API URL (“https://www.dhi.ac.uk/api/data/…”), and I could infer that it aggregates data into a CSV  file related to oldbailey_offences… but it isn’t mentioned anywhere in the API documentation.\nWe all like a good mystery, don’t we? Let’s have a quick look at what it fetches.\n\nlibrary(here) \nlibrary(janitor)\nlibrary(glue)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(listviewer)\nlibrary(tidyverse)\n\nobo_offence_category_stats_csv &lt;-\n  read_csv(\"https://www.dhi.ac.uk/api/data/csv/agg/oldbailey_offence?series=offence_category\")\n\nThis looks nice. There’s a key column with what I know are our offence categories’ names and a doc_count column, presumably counts of results. Counting what though? The default for statistical offence searches is to count by offence, so that’s the most likely (and is confirmed on the site results page).\n\nobo_offence_category_stats_csv\n\n# A tibble: 9 × 2\n  key           doc_count\n  &lt;chr&gt;             &lt;dbl&gt;\n1 breakingPeace      7696\n2 damage             1054\n3 deception         15879\n4 kill               5378\n5 miscellaneous      3233\n6 royalOffences     10681\n7 sexual             7224\n8 theft            151011\n9 violentTheft       9016\n\n\nLet’s test it out by running breakingPeace through an API search query.\n\nbreaking_peace_json &lt;- fromJSON(\"https://www.dhi.ac.uk/api/data/oldbailey_record?offence=breakingPeace&div_type=trialAccount\")\n\nThat gets, as anticipated, the first ten results for offences in the top level category of Breaking Peace. The total trial hits (7465) are lower than the doc_count (7696).\n\njsonedit(breaking_peace_json$hits)"
  },
  {
    "objectID": "notes/statistics-terms.html#digging-deeper",
    "href": "notes/statistics-terms.html#digging-deeper",
    "title": "API terms and statistics",
    "section": "Digging deeper",
    "text": "Digging deeper\nI’m calling these agg queries to differentiate them from the API search and single record queries I’ve already written about.\nAfter various tests and snafus, I worked out the key components of the URLs:\n\nthe fixed part of the URL is https://www.dhi.ac.uk/api/data/csv/agg/\nthe unit to count by is in the next path segment before the query string, eg /oldbailey_offence; it’s always prefixed by “oldbailey_”\nthe important bits of the query string are series and rows\n\nNB: you may also see a countby in query strings when exploring searches. I don’t advise using it in API queries; I tested it out quite a bit and I think it either doesn’t do anything at all or the oldbailey_* path parameter always takes precedence.\n\nwhat to count by?\nI should emphasise that what you count by in OBO statistical searches can matter a lot.\nThere are in fact six options: offence, verdict, punishment, defendant, victim and trial. Why so many?!\nThe OBO data is not simple tabular data. In any given trial, there can be multiple defendants, offences, verdicts, punishments and victims. So you can get (as seen with the breakingPeace query) very different results when you change what you count by, and the most appropriate choice is likely to depend on what kind of search or analysis you’re doing.\nWe spent quite a while, way back in about 2008, discussing the most appropriate countby defaults for each search; if you run a few queries on the site changing the search category but keeping the default counts, you can see how it changes. The complexity of the statistics tool led us to write a guide to its use.\nIt doesn’t become any less complex just because you interact it with it via the API rather than the search form on the site. In some respects it needs even more care, because searches on the site return much more information about the search (in the display it tells you exactly what was counted and what it was counted by). The agg query downloads lose much of that context. Plus, when constructing an agg query, you have to specify what you want to count by; you can’t just let the site do it for you.\nLet’s look again at the offence categories query, and this time change to /oldbailey_trial. Now the total for breakingPeace matches the json hits total.\n\nobo_offence_trial_query &lt;-\n  read_csv(\"https://www.dhi.ac.uk/api/data/csv/agg/oldbailey_trial?series=offence_category\")\n\nobo_offence_trial_query\n\n# A tibble: 9 × 2\n  key           doc_count\n  &lt;chr&gt;             &lt;dbl&gt;\n1 breakingPeace      7465\n2 damage             1027\n3 deception         14116\n4 kill               5301\n5 miscellaneous      3163\n6 royalOffences     10477\n7 sexual             6943\n8 theft            142539\n9 violentTheft       8628\n\n\n\nbreaking_peace_json$hits$total\n\n[1] 7465\n\n\nIn summary:\nThe one time it shouldn’t matter too much what you count by is if you simply want a list of the available terms for a search query.\nIf you want counts that will match the hits totals returned by the search API, use /oldbailey_trial. (You’ll also need to add div_type=trialAccount to the search query to ensure they match exactly.)\nIf you want counts for a statistical analysis and you want your results to be reproducible, choose the most appropriate count carefully and document your choice every time.\n\n\nquery strings\nIf you’re looking at the statistics search form it has four main sections (apart from filtering options in the sidebar, which I’m not going into here but would expect to work in much the same way as other API queries):\n\nselect a search category (required)\nselect a second category (optional) - adding this generates more complex tables (crosstabs)\ncount by (optional)\ndisplay output (optional)\n\n1 and 2 use identical lists of variables, which I’ve pulled out of the search form for convenience.\n\nobo_agg_variables_csv &lt;-\n  read_csv(here::here(\"data/obo_agg_variables.csv\"))\n\nobo_agg_variables_csv$name\n\n [1] \"offence_category\"       \"offence_subcategory\"    \"plea\"                  \n [4] \"verdict_category\"       \"verdict_subcategory\"    \"punishment_category\"   \n [7] \"punishment_subcategory\" \"defendant_gender\"       \"defendant_age\"         \n[10] \"victim_gender\"          \"victim_age\"             \"victim_hisco_class_1\"  \n[13] \"victim_hisco_label\"     \"victim_institution\"     \"decade\"                \n[16] \"year\"                  \n\n\nTranslating from the search form to agg query strings, the first search box is equivalent to series and the second, if you want it, is rows. As far as I can tell from testing you’ll never need anything else. You can have any combination of variables you like in crosstabs (though some are likely to make more sense than others).\n\n\na crosstabs example\nLet’s say I’d like a breakdown of offence subcategories per year, counting by offence.\n\nagg_query_url &lt;- \"https://www.dhi.ac.uk/api/data/csv/agg/oldbailey_offence?rows=offence_subcategory&series=year\"\n\noffence_subcategory_year_csv &lt;- read_csv(agg_query_url)\n\nThe result:\n\noffence_subcategory_year_csv\n\n# A tibble: 240 × 74\n     key doc_count animalTheft bigamy burglary coiningOffences grandLarceny\n   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n 1  1674        41           5      1       12               1            1\n 2  1675        59           4      0        8               1            7\n 3  1676        87           4      5       11               1            9\n 4  1677        91           6      2        8               5            7\n 5  1678       132          16      4        2               8           26\n 6  1679       131          10      0        6              10            8\n 7  1680       127           5      1       16               9           21\n 8  1681       135           8      3       11               2           22\n 9  1682       174           5      4        7               5           34\n10  1683       215          17      3        5              10           34\n# ℹ 230 more rows\n# ℹ 67 more variables: highwayRobbery &lt;dbl&gt;, housebreaking &lt;dbl&gt;,\n#   infanticide &lt;dbl&gt;, killOther &lt;dbl&gt;, murder &lt;dbl&gt;, rape &lt;dbl&gt;,\n#   receiving &lt;dbl&gt;, religiousOffences &lt;dbl&gt;, theftFromPlace &lt;dbl&gt;,\n#   theftOther &lt;dbl&gt;, arson &lt;dbl&gt;, assault &lt;dbl&gt;, forgery &lt;dbl&gt;,\n#   miscellaneousOther &lt;dbl&gt;, pervertingJustice &lt;dbl&gt;, pettyLarceny &lt;dbl&gt;,\n#   pettyTreason &lt;dbl&gt;, pocketpicking &lt;dbl&gt;, robbery &lt;dbl&gt;, …"
  },
  {
    "objectID": "notes/statistics-terms.html#scaling-up",
    "href": "notes/statistics-terms.html#scaling-up",
    "title": "API terms and statistics",
    "section": "Scaling up",
    "text": "Scaling up\nChances are you’ll want to get more than one thing at a time! Here are a couple of examples.\n\ndownload multiple CSVs\nMaybe you want to get the CSVs for all the search variables, ready to use in search scripts. Here’s one way to do it, for the simple series search for each variable.\nFirst build the query URLs and filenames.\n\n# construct a) URLs and b) filenames from the agg variables list\n\n# I like to embed info about the data in the filename: \n# name of variable + count by + date of download\n\ndate_stamp &lt;- today() |&gt; format('%Y%m%d')\n\nobo_agg_variables &lt;-\nobo_agg_variables_csv |&gt;\n  mutate(url = glue(\"https://www.dhi.ac.uk/api/data/csv/agg/oldbailey_trial?series={name}\")) |&gt;\n  mutate(filename = glue(\"{name}-bytrial-{date_stamp}\"))\n\n# pull out the variable names\nobo_agg_variables_names &lt;- obo_agg_variables$name\n\n# pull out the filenames\nobo_agg_variables_filenames &lt;- obo_agg_variables$filename\n\nThen the process for fetching and saving is much the same as I used in my previous API search example\n\n# function to write the CSVs to files\n# the outputs/csv/single/ folder must already exist\noutput_obo_single_csv &lt;- function(data, names) {\n    folder_path &lt;- here::here(\"outputs/csv/single\")\n    write_csv(data, paste0(folder_path, \"/\", names, \".csv\") )\n}\n\n# slightly slow down read_csv with slowly() when it's used in map.\nslow_csv &lt;-\n  slowly(read_csv, rate=rate_delay(1))\n\n# use purrr::map to fetch the data for each variable\nobo_agg_variables_csvs &lt;- map(obo_agg_variables$url, slow_csv)\n\n# add the variable names back to the list elements\nnames(obo_agg_variables_csvs) &lt;- obo_agg_variables_names\n\n# write the CSVs with purrr::pmap.\ninvisible(\n  list(\n    data=obo_agg_variables_csvs,\n    names=obo_agg_variables_filenames\n  ) |&gt;\n  pmap(output_obo_single_csv)\n )\n\nChecking out one of the files:\n\nobo_agg_punishment_category_csv &lt;-\n  read_csv(here::here(\"outputs/csv/single/punishment_category-bytrial-20250529.csv\"))\n\nobo_agg_punishment_category_csv\n\n# A tibble: 6 × 2\n  key        doc_count\n  &lt;chr&gt;          &lt;dbl&gt;\n1 corporal        8910\n2 death          10486\n3 imprison       80937\n4 miscPunish     12492\n5 noPunish        4838\n6 transport      40521\n\n\n\n\nall the combinations for crosstabs\nThere is a neat R function, combn(), to make a set of all unique pairs from a list, when you want to avoid reverse-duplicates (ie, you don’t need both “a”+“b” and “b”+“a”). The only downside is that it returns a matrix, so you need to convert back to a tibble/dataframe.\n\nobo_agg_variables_pairs &lt;-\ncombn(obo_agg_variables_names, m=2 ) |&gt;\n  # it's a *very* wide matrix; t() transposes rows and columns\n  t() |&gt;\n  # convert matrix to tibble; .name_repair to create column names\n  as_tibble(.name_repair = ~c(\"series\", \"rows\"))\n\n\nobo_agg_variables_pairs\n\n# A tibble: 120 × 2\n   series           rows                  \n   &lt;chr&gt;            &lt;chr&gt;                 \n 1 offence_category offence_subcategory   \n 2 offence_category plea                  \n 3 offence_category verdict_category      \n 4 offence_category verdict_subcategory   \n 5 offence_category punishment_category   \n 6 offence_category punishment_subcategory\n 7 offence_category defendant_gender      \n 8 offence_category defendant_age         \n 9 offence_category victim_gender         \n10 offence_category victim_age            \n# ℹ 110 more rows\n\n\nOn the other hand, if you want all the possible combinations including reverse-duplicates, you could instead use tidyr::expand_grid() which will “create a tibble from all combinations of inputs”. That’ll include identical pairs, so you need to filter those out.\n\nobo_agg_variables_pairs_all &lt;-\n  expand_grid(rows=obo_agg_variables_names, \n              series=obo_agg_variables_names) |&gt;\n  filter(rows != series)\n\n\nobo_agg_variables_pairs_all\n\n# A tibble: 240 × 2\n   rows             series                \n   &lt;chr&gt;            &lt;chr&gt;                 \n 1 offence_category offence_subcategory   \n 2 offence_category plea                  \n 3 offence_category verdict_category      \n 4 offence_category verdict_subcategory   \n 5 offence_category punishment_category   \n 6 offence_category punishment_subcategory\n 7 offence_category defendant_gender      \n 8 offence_category defendant_age         \n 9 offence_category victim_gender         \n10 offence_category victim_age            \n# ℹ 230 more rows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Old Bailey Online for Data Analysis",
    "section": "",
    "text": "This website is a resource for users of the Old Bailey Online who are interested in Digital Humanities, Data Science and computational approaches to the OBO’s complex data.\nI plan to write guides to using the new (2023) OBO API, as well as useful tools and methods for processing data obtained via the API. Caveat: some of this is work in progress; I’m still exploring the API and finding new features. Users should verify for themselves that my advice works for them.\nCode and data will also be available from the Github repository"
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Old Bailey Online for Data Analysis",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website is a resource for users of the Old Bailey Online who are interested in Digital Humanities, Data Science and computational approaches to the OBO’s complex data.\nInitially, it will contain reference guides to using the new (2023) OBO API. I’d like to include material about useful tools and methods for processing data obtained via the API. And then we’ll see.\nCode and data will also be available from the Github repository"
  },
  {
    "objectID": "about.html#about-this-website",
    "href": "about.html#about-this-website",
    "title": "About",
    "section": "",
    "text": "This website is a resource for users of the Old Bailey Online who are interested in Digital Humanities, Data Science and computational approaches to the OBO’s complex data.\nInitially, it will contain reference guides to using the new (2023) OBO API. I’d like to include material about useful tools and methods for processing data obtained via the API. And then we’ll see.\nCode and data will also be available from the Github repository"
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About",
    "section": "About the author",
    "text": "About the author\nSharon Howard is a digital historian who has been involved with the Old Bailey Online project for almost 20 years, as project manager, researcher and data manager."
  },
  {
    "objectID": "about.html#about-the-old-bailey-online",
    "href": "about.html#about-the-old-bailey-online",
    "title": "About",
    "section": "About the Old Bailey Online",
    "text": "About the Old Bailey Online\nThe Old Bailey Proceedings Online, first launched in 2003 with subsequent major updates in 2008 and 2023, makes available a fully searchable, digitised collection of all surviving editions of the Old Bailey Proceedings from 1674 to 1913, and of the Ordinary of Newgate’s Accounts between 1676 and 1772. It allows access to over 197,000 trials and biographical details of approximately 2,500 men and women executed at Tyburn, free of charge for non-commercial use. See the About the Project pages for more information about the project methods and evolution."
  },
  {
    "objectID": "notes/intro-api.html",
    "href": "notes/intro-api.html",
    "title": "The new OBO API: an overview",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were major breaking changes to the OBO API. The available documentation for the new API is sparse, and these notes are intended to help users find their way around.\nI cover two of the available API endpoints here:\n\nhttps://www.dhi.ac.uk/api/data/oldbailey_record - for searching trials and other sections in the Old Bailey Proceedings\nhttps://www.dhi.ac.uk/api/data/oldbailey_record_single - for full data about individual trials and other sections in the Proceedings\n\nThere are two other documented endpoints, one for Ordinary’s Accounts and one for the Associated Records database. I haven’t yet looked at these in any detail and may add further notes about them later, but would expect them to work in similar ways.\nThere is a further undocumented feature for obtaining aggregated data. I’ll cover that in a separate post.\nSee also this worked example to search and download data from the API using R.\nFor further reference:\n\ndetailed background information about the search categories (offences, verdicts, sentences/punishments, victims and defendants) can be found on the OBO website.\nall the OBO search forms have extensive contextual help which is also relevant for understanding the workings of the API."
  },
  {
    "objectID": "notes/intro-api.html#introduction",
    "href": "notes/intro-api.html#introduction",
    "title": "The new OBO API: an overview",
    "section": "",
    "text": "As part of a major overhaul and refresh of the Old Bailey Online in 2023, there were major breaking changes to the OBO API. The available documentation for the new API is sparse, and these notes are intended to help users find their way around.\nI cover two of the available API endpoints here:\n\nhttps://www.dhi.ac.uk/api/data/oldbailey_record - for searching trials and other sections in the Old Bailey Proceedings\nhttps://www.dhi.ac.uk/api/data/oldbailey_record_single - for full data about individual trials and other sections in the Proceedings\n\nThere are two other documented endpoints, one for Ordinary’s Accounts and one for the Associated Records database. I haven’t yet looked at these in any detail and may add further notes about them later, but would expect them to work in similar ways.\nThere is a further undocumented feature for obtaining aggregated data. I’ll cover that in a separate post.\nSee also this worked example to search and download data from the API using R.\nFor further reference:\n\ndetailed background information about the search categories (offences, verdicts, sentences/punishments, victims and defendants) can be found on the OBO website.\nall the OBO search forms have extensive contextual help which is also relevant for understanding the workings of the API."
  },
  {
    "objectID": "notes/intro-api.html#notes-on-search-parameters",
    "href": "notes/intro-api.html#notes-on-search-parameters",
    "title": "The new OBO API: an overview",
    "section": "Notes on search parameters",
    "text": "Notes on search parameters\nThe official documentation only mentions keyword searching with the text parameter. These notes are intended to fill in some of the gaps for more structured searches of offences, etc.\nThe crucial thing to know is that API URLs query strings mirror search results URLs on the website.\nSo, for example, the URL for the first page of results for a site search for the offence “killing”:\n\nhttps://www.oldbaileyonline.org/search/crime?offence=kill\n\nThe corresponding api query url:\n\nhttps://www.dhi.ac.uk/api/data/oldbailey_record?offence=kill\n\nNote that the /crime/ segment of the website URL is not needed.\nSo generally you should be able to test out queries on the website first to work out what parameters you need for API queries, and I’m not going to list a lot of examples here. There could potentially be specific omissions/variations in the API that I’m not aware of, so I do recommend generally testing out results for whatever you want to do.\n\npagination\nThe example URL above will only get the first 10 results, but most searches return more than that. The parameter for subsequent pages is from. Numbering starts from 0, not 1.\nEg, to get the second page of results: from=10\nSo, the second page for the example above: https://www.dhi.ac.uk/api/data/oldbailey_record?offence=kill&from=10\n\n\ndates and date ranges\nA range specifying month and year (March 1739-August 1755): month_gte=3&month_lte=8&year_gte=1739&year_lte=1755\nFor the year range only: year_gte=1739&year_lte=1755\nA single year: year_gte=1739&year_lte=1739\n\n\noffences\nThe top level category breaking peace (including all subcategories): offence=breakingPeace\nFor wounding, subcategory of breakingPeace: offence=wounding\nFor subcategories you don’t need to specify the parent category except for “other” subcategories\n\noffence=breakingPeaceOther / offence=deceptionNoDetail\n\n“NoDetail” is a new subcategory which essentially means the same thing as “Other” but reflects some inconsistencies in the XML where the offence was tagged without any subcategory at all (but should properly have been Other). In the old API I think there was no way to pull these out separately.\n\n\nverdicts and sentences\nA verdict of guilty: verdict=guilty\nA sentence of death: punishment=death\n\n\npleas (new)\nPleaded guilty: plea=guilty\nI think all the old “pleaded” verdicts are still available under verdict and on a superficial inspection seem to get much the same results. Eg plea=guilty and verdict=pleadedGuilty seem to get the same results.\nHowever, this new option is the result of a fairly recent project and there may well be details I don’t know about in the background information on the website.\n\n\nitem types\nTo restrict to trial texts only (ie excluding advertisements, supplementary, etc): div_type=trialAccount\nThis is likely to make fairly slight differences when doing structured offence/verdict/sentence queries but will be more relevant for text queries. It’s recommended if you want to be absolutely consistent and certain about where results come from.\n\n\ndefendants and victims\nEg for gender:\n\ndefendant_gender=female\nvictim_gender=male"
  },
  {
    "objectID": "notes/intro-api.html#the-json",
    "href": "notes/intro-api.html#the-json",
    "title": "The new OBO API: an overview",
    "section": "The JSON",
    "text": "The JSON\nThere is a lot of… stuff… in the JSON returned by the API, much of it not useful for data analysis. This focuses on some bits you might care about.\n\nsearch results\nBrowse a sample of the search results JSON:\n\njsonedit(sample_search_json)\n\n\n\n\n\n(search for housebreaking between 1700 and 1704)\nThe number of results (hits &gt; total)\n\nsample_search_json$hits$total\n\n[1] 14\n\n\nTrial IDs (hits &gt; hits &gt; _source &gt; idkey) - needed to get full data of individual trials via the single record endpoint.\n\nsample_search_json$hits$hits$`_source`$idkey\n\n [1] \"t17000115-17\" \"t17000115-21\" \"t17000828-4\"  \"t17000828-13\" \"t17000828-14\"\n [6] \"t17000828-28\" \"t17000828-29\" \"t17021014-5\"  \"t17021014-9\"  \"t17021014-19\"\n\n\nThe search returns very limited information about each trial (or other item); it does include a text snippet and the page title for each item, which could be useful for some purposes.\n\nsample_search_json$hits$hits$`_source`$text[[1]]\n\n[1] \"Anne Buckler and Elizabeth Jefferys , both of the Parish of St. Andrews Holborne , the first was Indicted as Principal, and the other as Accessary before the Fact committed, for breaking the House of Thomas Beckenson , on the 9th of December last, with an Intention to steal his Goods . The Evidence not being sufficient to Convict the Principal they were both acquitted .\"\n\n\n\nsample_search_json$hits$hits$`_source`$title[[1]]\n\n[1] \"Anne Buckler. Elizabeth Jefferys. Theft; housebreaking. 15th January 1700.\"\n\n\n\n\nsingle records\nThe single record endpoint is needed to get the full data about items.\nIt includes three versions of the trial/item text; two are most likely to be of interest for data analysis.\n\nxml - the full tagged XML (hits &gt; hits &gt; _source &gt; xml)\ntext - a plain text version (hits &gt; hits &gt; _source &gt; text)\n\nplus\n\nmetadata - this contains the same information as the simplified summary table at the top of each trial page, which might be sufficient for some purposes (hits &gt; hits &gt; _source &gt; metadata)\n\nBrowse a sample trial JSON:\n\njsonedit(sample_trial_json)\n\n\n\n\n\n\n## sample_trial_json$hits$hits$`_source`$xml"
  },
  {
    "objectID": "notes/intro-api.html#for-users-of-the-old-obapi",
    "href": "notes/intro-api.html#for-users-of-the-old-obapi",
    "title": "The new OBO API: an overview",
    "section": "For users of the old OBAPI",
    "text": "For users of the old OBAPI\nA few notes about the major differences between new and old API.\n\nchanges to URL query strings\nPreviously for offences/verdicts/sentences in the URL it was always necessary to explicitly spell out category_subcategory. That’s no longer necessary except in a few specific contexts (and it’s done slightly differently).\nAn example: offence “fraud”, subcategory of deception. The relevant bit of a search URL previously looked like this:\n_offences_offenceCategory_offenceSubcategory=deception_fraud\nThat’s been replaced by the much shorter and simpler\noffence=fraud\nNow the top level category is only needed if\n\nsearching for the whole category, eg: offence=deception\nsearching for subcategories Other or NoDetail, which need to be like this\n\noffence=deceptionOther (previously deception_other)\noffence=deceptionNoDetail (not in the previous version)\n\n\n\n\nwhat’s gone away\nThe new API is in many ways more powerful than the old and with fewer limits, but it’s also more generic and some functionality is no longer available (as far as I can tell, though the lack of documentation can make things hard to find).\n\nthe terms endpoint which gave a list of all the fields and values available in the API\nthe option (breakdown=[field]) for a breakdown of subcategories and so on for a search\nthe option (return=zip) to download zip archives of multiple trials’ XML\n\nThe functionality can be reproduced but usually with some loss of convenience; if you have scripts using them you’ll probably find that their replacements need to be longer and more complicated.\nThe OBAPI demonstrator has also been withdrawn; this has no replacement.\nIt used to be possible to specify the date of a particular session of the Proceedings (in fact, session dates may have been the only way to limit by date, though my memory is fuzzy on this). That no longer seems to be an option; there’s only month and year. Month-year can often be used as a proxy for session date, but isn’t completely perfect; after 1834 there could be more than one session in the same month."
  }
]